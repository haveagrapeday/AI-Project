import tensorflow as tf
import os
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from collections import Counter
from PIL import Image

def clean_dataset(data_dir):
    """ ‡πÅ‡∏õ‡∏•‡∏á‡∏†‡∏≤‡∏û‡∏û‡∏≤‡πÄ‡∏•‡∏ï‡πÄ‡∏õ‡πá‡∏ô RGBA ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢ """
    for root, _, files in os.walk(data_dir):
        for file in files:
            file_path = os.path.join(root, file)
            try:
                img = Image.open(file_path)
                img.verify()
                if img.mode == "P":
                    img = img.convert("RGBA")
                    img.save(file_path)
            except Exception as e:
                print(f"‚ö†Ô∏è Removing corrupted image: {file_path}")
                os.remove(file_path)

def load_data(data_dir, img_size, batch_size):
    """ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ Augmentation """
    clean_dataset(data_dir)
    datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=30,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        validation_split=0.2
    )
    train_generator = datagen.flow_from_directory(
        data_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', subset='training')
    val_generator = datagen.flow_from_directory(
        data_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', subset='validation')
    return train_generator, val_generator

def compute_class_weights(generator):
    """ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™ ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏• """
    counter = Counter(generator.classes)
    max_count = max(counter.values())
    class_weights = {cls: max_count / count for cls, count in counter.items()}
    return class_weights

def build_model(input_shape, num_classes):
    """ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ResNet50 ‡∏û‡∏£‡πâ‡∏≠‡∏° Fine-tuning """
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
    for layer in base_model.layers[:-30]:  # Unfreeze ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 30 ‡∏ä‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        layer.trainable = False
    
    x = GlobalAveragePooling2D()(base_model.output)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.4)(x)
    output_layer = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs=base_model.input, outputs=output_layer)
    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

def main():
    DATA_DIR = "datasources/princess"
    IMG_SIZE = (224, 224)
    BATCH_SIZE = 16
    EPOCHS = 30
    MODEL_PATH = "disney_princess_resnet50_best.h5"
    
    print("üìÇ Loading data...")
    train_generator, val_generator = load_data(DATA_DIR, IMG_SIZE, BATCH_SIZE)
    print(f"üìä Classes found: {train_generator.class_indices}")
    
    class_weights = compute_class_weights(train_generator)
    
    print("üõ†Ô∏è Building model..")
    model = build_model((IMG_SIZE[0], IMG_SIZE[1], 3), len(train_generator.class_indices))
    
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
        ModelCheckpoint(MODEL_PATH, monitor='val_accuracy', save_best_only=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)
    ]
    
    print("üöÄ Training model...")
    model.fit(train_generator, epochs=EPOCHS, validation_data=val_generator, class_weight=class_weights, callbacks=callbacks)
    print("‚úÖ Training complete!")
    print(f"üíæ Model saved at {MODEL_PATH}")
    
if __name__ == "__main__":
    main()

